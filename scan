#!/usr/bin/env python
import requests, re, json, base64, html, random, sys, time, copy, os
from bs4 import BeautifulSoup
from pwn import log
import jsbeautifier




regex_str = r"""
  (?:"|')                               # Start newline delimiter
  (
    ((?:[a-zA-Z]{1,10}://|//)           # Match a scheme [a-Z]*1-10 or //
    [^"'/]{1,}\.                        # Match a domainname (any character + dot)
    [a-zA-Z]{2,}[^"']{0,})              # The domainextension and/or path
    |
    ((?:/|\.\./|\./)                    # Start with /,../,./
    [^"'><,;| *()(%%$^/\\\[\]]          # Next character can't be...
    [^"'><,;|()]{1,})                   # Rest of the characters can't be
    |
    ([a-zA-Z0-9_\-/]{1,}/               # Relative endpoint with /
    [a-zA-Z0-9_\-/]{1,}                 # Resource name
    \.(?:[a-zA-Z]{1,4}|action)          # Rest + extension (length 1-4 or action)
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
    |
    ([a-zA-Z0-9_\-/]{1,}/               # REST API (no extension) with /
    [a-zA-Z0-9_\-/]{3,}                 # Proper REST endpoints usually have 3+ chars
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
    |
    ([a-zA-Z0-9_\-]{1,}                 # filename
    \.(?:php|asp|aspx|jsp|json|
         action|html|js|txt|xml)        # . + extension
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
  )
  (?:"|')                               # End newline delimiter
"""



def user_agent():
    user_agent_list = [
            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 OPR/85.0.4341.71',
            'Mozilla/5.0 (Windows; U; Windows NT 5.2; es-VE) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36 UCBrowser/13.4.0.1306',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.101 Safari/537.36 Edg/104.0.1293.70',
            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.127 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4476.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.5195.79 Safari/537.36 OPR/91.0.4516.78',
            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Safari/537.36 Edg/90.0.818.42',
            
    ]
    user_agent = random.choice(user_agent_list)
    return user_agent


def scrap(link, content, js=False):
    
    
    
    if js:
            
        
        if len(content) > 1000000:
            print("getting js file "+link)
            content = content.replace(";",";\r\n").replace(",",",\r\n")
        else:
            #Beautify 
            content = jsbeautifier.beautify(content)
            print("getting js file beaufify "+link)
            
      
       
        filename = "js/" + os.path.basename(link)
        with open(filename, "w") as js_file:
            js_file.write(content)
                
        
        

    regex = re.compile(regex_str, re.VERBOSE)
    for m in re.finditer(regex, content):
        if m.group(1) not in links["full_urls"] and m.group(1) is not None:
            links["full_urls"].append(m.group(1))
        if m.group(2) not in links["path_trv"] and m.group(2) is not None:
            links["path_trv"].append(m.group(2))
        if m.group(3) not in links["endpoint"] and m.group(3) is not None:
            links["endpoint"].append(m.group(3))
        if m.group(4) not in links["relative_with_ext"] and m.group(4) is not None:
            links["relative_with_ext"].append(m.group(4))

   
    with open("data.json", "w") as file:
        json.dump(links, file)


links = {
    "full_urls": [],
    "path_trv": [],
    "endpoint": [],
    "relative_with_ext": [], 
}
if os.path.isfile("data.json"):
    with open("data.json", "r") as file:
        links = json.load(file)
        

if __name__ == "__main__":
    
    url = sys.argv[1]
    
    cookies = {}
    if len(sys.argv) == 3:
        cookies = json.loads(sys.argv[2])
        

    
    
    content = requests.get(url, headers={"User-Agent": user_agent()}, cookies=cookies).text    
    scrap(url, content, js=False)
    
    s = log.progress(f"Scanning {url}")
    
    scanned = 0
    links2scan = copy.deepcopy(links["endpoint"])
    #Scrap the adquired endpoints
    for link in links2scan:
        if len(links2scan) > 1000:
            continue
        scanned += 1
        
        try:
            resp = requests.get(url+link, headers={"User-Agent": user_agent()}, cookies=cookies, timeout=1)
            content2 = resp.text
        except:
            continue
        
        js = False
        if "javascript" in resp.headers["Content-Type"].lower() or link.endswith(".js") or ".js?" in link:
            js = True
            
        
        scrap(link, content2, js)
    
        s.status(f"{scanned}/{len(links2scan)}")
    
    
    
    
    links2scan = copy.deepcopy(links["full_urls"])
    for link in links2scan:
        if link.endswith(".js") or ".js?" in link or link.endswith(".map"):
            if "http" not in link:
                link = "https:"+link
                
            try:    
                response = requests.get(link, headers={"User-Agent": user_agent()}, cookies=cookies).text
                scrap(link, response, js=True)       
                
            except:
                continue
    
    
